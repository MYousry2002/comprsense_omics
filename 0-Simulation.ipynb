{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf419a21",
   "metadata": {},
   "source": [
    "# Compressed Sensing Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1775d0",
   "metadata": {},
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5928922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import spams\n",
    "from scipy.stats import entropy\n",
    "from scipy.spatial import distance\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.sparse as sp\n",
    "from scipy.io import mmread\n",
    "from scipy.stats import spearmanr, pearsonr, entropy\n",
    "from scipy.spatial import distance\n",
    "import os\n",
    "\n",
    "THREADS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebadedca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24409, 611034)\n",
      "611034\n",
      "24409\n",
      "AnnData object with n_obs × n_vars = 611034 × 24409\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [Xkr4, Gm1992, Gm37381, Rp1, Sox17, Mrpl15, Lypla1, Gm37988, Tcea1, Rgs20, Atp6v1h, Rb1cc1, 4732440D04Rik, Fam150a, St18, Pcmtd1, Gm26901, Sntg1, Rrs1, Adhfe1, Mybl1, Vcpip1, 1700034P13Rik, Sgk3, Mcmdc2, Snhg6, Tcf24, Ppp1r42, Gm15818, Cops5, Cspp1, Arfgef1, Cpa6, Prex2, A830018L16Rik, Sulf1, Slco5a1, Gm29283, Prdm14, Ncoa2, Gm29570, Tram1, Lactb2, Eya1, Trpa1, Kcnb2, Terf1, Sbspon, 4930444P10Rik, Rpl7, Rdh10, Gm28095, Stau2, Gm7568, Ube2w, Tceb1, D030040B21Rik, Tmem70, Ly96, Gm28376, Jph1, Gdap1, Pi15, Gm28154, Gm16070, Crispld1, Gm28153, Defb41, Tfap2d, Tfap2b, Pkhd1, Il17f, Mcm3, 6720483E21Rik, Paqr8, Efhc1, Tram2, Tmem14a, Gsta3, Gm28836, Kcnq5, Rims1, Gm29107, Ogfrl1, Gm28822, B3gat2, Smap1, Sdhaf4, Fam135a, Col9a1, Col19a1, Lmbrd1, Adgrb3, Phf3, Ptp4a1, Gm29669, Lgsn, Khdrbs2, Prim2, Rab23, ...]\n",
      "\n",
      "[24409 rows x 0 columns]\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [IXa_M003_TTTACTGGTACAGTAA, IXa_M003_AACCTTTAGCGACTTT, IXa_M003_TGTGCGGAGGTCGACA, IXa_M003_CTGTGAAGTAACAGTA, IXa_M003_GCGTTTCAGACTCAAA, IXa_M003_CAGCAGCCAACCGCCA, IXa_M003_AGTTCGACATCCAATG, IXa_M003_GAGTCTATCGGAACTT, IXa_M003_ATTTCTGAGCTCTTCC, IXa_M003_TCTTTGAAGAGGGTAA, IXa_M003_AGAGCAGTCGCCAGTG, IXb_M003_CTACCTGGTGAAGCTG, IXb_M003_TGTTCATCATGACACT, IXb_M003_GTGGTTACAGCGACAA, IXb_M003_AAACCCACATCGGATT, IXb_M003_GAGACCCCACATGTTG, IXb_M003_GGTGTTAAGAAGTATC, IXb_M003_CACTGTCGTGACACAG, IXb_M003_TCATCCGAGCACTAGG, IXc_M003_GGTGAAGAGCATTGAA, IXc_M003_CATGAGTCACAGTACT, IXc_M003_GCTTCACCACAGACGA, IXc_M003_TACCGGGGTTATGGTC, IXc_M003_TCAAGTGTCAACGTGT, IXc_M003_GAGCTGCCATGACTAC, IXc_M003_GATAGCTAGCTAAACA, IXc_M003_CGGGACTTCGTAGTCA, IXc_M003_TCTTGCGTCCAGTACA, IXc_M003_TTCAATCAGGAGATAG, IXc_M003_TGGGATTGTCCTGAAT, IXb_M003_AGTTAGCTCATAGAGA, PFa_M001_AGCTCAAAGTCTGTAC, PFa_M001_CGTAAGTCAGATCATC, PFa_M001_CCAATGAAGTTGCTCA, PFa_M001_ACTTCCGAGGTTATAG, PFa_M001_TTACCATAGTTAACAG, PFa_M001_CCGAACGAGAGCAGAA, PFa_M001_TTCTAGTTCGTGGTAT, PFa_M001_GGATCTATCGCCTTTG, PFb_M001_TCATGCCTCCGTTTCG, PFb_M001_ACCTGTCTCCACGTAA, PFb_M001_ACCTGTCAGAAATTGC, PFb_M001_AACTTCTCACCTGCGA, PFb_M001_TGGGAGAAGACATCAA, PFb_M001_TGCTCCAGTTGCGAAG, PFb_M001_AAGCGTTGTAAGTAGT, PFb_M001_TCCGATCGTAACGATA, PFb_M001_ACGGAAGAGCACCCAC, PFc_M001_AACTTCTGTATGGTTC, PFc_M001_AGCCAATGTCATCGGC, PFc_M001_CAAGGGAGTTCTCTCG, PFc_M001_GTAACACCATCCGAAT, PFc_M001_ATGCCTCTCCACGGAC, PFc_M001_AGCGTATGTTTCGGCG, PFd_M001_TCGAAGTTCGCTCTAC, PFd_M001_GTGGAGAAGTTTCGGT, Ia_M001_CTCAAGAAGGGCGAGA, Ia_M001_GGGACTCCATCACCAA, Ia_M001_CCTATCGTCTATACTC, Ia_M001_TTACGTTCACCAGTAT, Ia_M001_TCGCACTGTCTCTCTG, Ia_M001_TGTAACGTCTGGACCG, Ia_M001_ATCGGCGGTCAAGGCA, Ia_M001_CTGTACCGTGAAAGTT, Ia_M001_GTTCTATAGGTAAGAG, Ia_M001_CCTTTGGCAGTCAGAG, Ib_M001_GTGTAACGTAACACGG, Ib_M001_CACTGAAAGTTGCCCG, Ib_M001_CTTACCGCAAAGACGC, Ib_M001_GGAATCTCACCAGCCA, Ib_M001_TTCCAATGTGTTCATG, Ib_M001_CGAAGTTTCGAACCAT, Ib_M001_CAACAACGTAGCCAGA, Ib_M001_TCTAACTCATCTAACG, Ib_M001_GTTGTAGGTCATGGCC, Ib_M001_ACTTCGCTCCCAAGTA, VIIIa_M001_CTTCTCTGTTCTGACA, VIIIa_M001_CTCACTGGTCTTGGTA, VIIIa_M001_AGTAGCTCAACTGCCG, VIIIa_M001_TGGAACTTCGCTTTAT, VIIIa_M001_CACTGGGGTGTCATCA, VIIIa_M001_GACTATGAGCTGTCCG, VIIIa_M001_TCGACCTGTCGAGTTT, VIIIa_M001_CACGTTCGTAACATCC, VIIIa_M001_ACCTACCCAACAAGAT, VIIIb_M001_ATCCTATGTGTGCTTA, VIIIb_M001_CCTACGTAGCTGCCAC, VIIIb_M001_AGTACCAAGCAGCGAT, VIIIb_M001_CATAGACAGCCACAAG, VIIIb_M001_TGTAGACGTCCGAAGA, VIIIb_M001_TGGAGGACACGATAGG, VIIIb_M001_GGTCTGGAGCTTCATG, VIIIb_M001_CAGCAATCAAATCGGG, VIIIb_M001_CAGCGTGTCATGCCGG, VIIIb_M001_TCAATTCGTGCGTGCT, IIa_M001_TTGACCCGTCCCTGTT, IIa_M001_ACGTACATCAGTGTTG, IIa_M001_CCACACTGTGCACGCT, IIa_M001_GTAAGTCCAGCTGTAT, IIa_M001_TGTGTGATCTCGTCGT, ...]\n",
      "\n",
      "[611034 rows x 0 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define file paths\n",
    "matrix_file = \"dataset/GSE165371_cb_adult_mouse/cb_adult_mouse.mtx.gz\"\n",
    "barcodes_file = \"dataset/GSE165371_cb_adult_mouse/cb_adult_mouse_barcodes.txt\"\n",
    "genes_file = \"dataset/GSE165371_cb_adult_mouse/cb_adult_mouse_genes.txt\"\n",
    "\n",
    "# Load the data\n",
    "matrix = mmread(matrix_file).tocsc()  # Read the matrix market file\n",
    "barcodes = pd.read_csv(barcodes_file, header=None).iloc[:, 0].tolist()  # Read barcodes\n",
    "genes = pd.read_csv(genes_file, header=None).iloc[:, 0].tolist()  # Read genes\n",
    "\n",
    "\n",
    "print(matrix.shape)\n",
    "print(len(barcodes))  # Number of barcodes\n",
    "print(len(genes))     # Number of genes\n",
    "\n",
    "\n",
    "# AnnData creation\n",
    "adata = sc.AnnData(\n",
    "    X=matrix.T,  # Transpose the matrix to match AnnData's format\n",
    "    obs=pd.DataFrame(index=barcodes),  # Barcodes -> obs\n",
    "    var=pd.DataFrame(index=genes)      # Genes -> var\n",
    ")\n",
    "\n",
    "print(adata)\n",
    "\n",
    "# Save the AnnData object to an H5AD file (optional)\n",
    "adata.write(\"dataset/cerebellum/cb_adult_mouse.h5ad\")\n",
    "\n",
    "# verify\n",
    "print(adata.var)\n",
    "print(adata.obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b39773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = sc.read_h5ad(\"dataset/cerebellum/cb_adult_mouse.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d92f28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def downsample_adata(adata, gene_set_size, cell_count=10000, output_path=\"dataset/cerebellum/cb_adult_mouse\"):\n",
    "    \"\"\"\n",
    "    Downsamples an AnnData object based on a specified gene set size.\n",
    "    \n",
    "    Parameters:\n",
    "    - adata: AnnData object to be downsampled\n",
    "    - gene_set_size: int, number of genes to include (500, 1000, or 5000)\n",
    "    - cell_count: int, number of cells to select (default: 10,000)\n",
    "    - output_path: str, base path for saving the downsampled file\n",
    "    \n",
    "    Returns:\n",
    "    - AnnData object with selected genes and cells\n",
    "    \"\"\"\n",
    "    assert gene_set_size in [500, 1000, 5000], \"Gene set size must be 500, 1000, or 5000.\"\n",
    "    \n",
    "    # Load genes from file\n",
    "    gene_file = f\"./dataset/genes_{gene_set_size}.csv\"\n",
    "    genes = pd.read_csv(gene_file, header=None)[1].tolist()\n",
    "    \n",
    "    # Normalize gene names\n",
    "    adata.var_names = adata.var_names.str.upper().str.strip()\n",
    "    genes = [gene.upper().strip() for gene in genes]\n",
    "    \n",
    "    # Match genes with existing ones in the dataset\n",
    "    selected_genes = [gene for gene in genes if gene in adata.var_names]\n",
    "    print(f\"Matched genes for {gene_set_size} set: {len(selected_genes)}\")\n",
    "    \n",
    "    # Ensure unique gene names\n",
    "    adata.var_names_make_unique()\n",
    "    \n",
    "    # Randomly select cells\n",
    "    np.random.seed(23)\n",
    "    selected_cells = np.random.choice(adata.obs_names, size=cell_count, replace=False)\n",
    "    \n",
    "    # Subset the AnnData object\n",
    "    adata_subset = adata[selected_cells, selected_genes]\n",
    "    \n",
    "    # Save the new dataset\n",
    "    output_file = f\"{output_path}_{gene_set_size}genes.h5ad\"\n",
    "    adata_subset.write(output_file)\n",
    "    print(f\"Downsampled dataset saved to {output_file}\")\n",
    "    \n",
    "    return adata_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db3c106f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_save_data(adata, dataset_prefix, output_dir=\"./dataset/\"):\n",
    "    \"\"\"\n",
    "    Splits `adata.X` into train, validation, and test subsets and saves them in the specified directory.\n",
    "    \n",
    "    Parameters:\n",
    "    - adata: AnnData object, containing the dataset.\n",
    "    - dataset_prefix: str, manually specified prefix for saved dataset files.\n",
    "    - output_dir: str, directory to save the subsets.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Ensure `adata.X` is in a compatible format\n",
    "    X = adata.X\n",
    "    \n",
    "    # Convert sparse matrices to compressed row format\n",
    "    if sp.issparse(X):  \n",
    "        X = X.tocsr()\n",
    "    else:\n",
    "        X = np.asarray(X, dtype=np.float64)\n",
    "    \n",
    "    # Get total number of samples (cells)\n",
    "    num_cells = X.shape[0]\n",
    "    \n",
    "    # Define split sizes (50% Train, 25% Validate, 25% Test)\n",
    "    train_idx, temp_idx = train_test_split(np.arange(num_cells), test_size=0.50, random_state=23)\n",
    "    validate_idx, test_idx = train_test_split(temp_idx, test_size=0.50, random_state=23)\n",
    "    \n",
    "    # Extract data using indices\n",
    "    train_data = X[train_idx]\n",
    "    validate_data = X[validate_idx]\n",
    "    test_data = X[test_idx]\n",
    "    \n",
    "    # Convert all subsets to dense arrays before saving\n",
    "    train_data = np.asarray(train_data.todense() if sp.issparse(train_data) else train_data, dtype=np.float64)\n",
    "    validate_data = np.asarray(validate_data.todense() if sp.issparse(validate_data) else validate_data, dtype=np.float64)\n",
    "    test_data = np.asarray(test_data.todense() if sp.issparse(test_data) else test_data, dtype=np.float64)\n",
    "\n",
    "    # Convert train_data to Fortran-contiguous format for SPAMS\n",
    "    train_data = np.asfortranarray(train_data)\n",
    "    \n",
    "    # Save subsets with manually inputted prefix\n",
    "    np.save(os.path.join(output_dir, f\"{dataset_prefix}_train_data.npy\"), train_data)\n",
    "    np.save(os.path.join(output_dir, f\"{dataset_prefix}_validate_data.npy\"), validate_data)\n",
    "    np.save(os.path.join(output_dir, f\"{dataset_prefix}_test_data.npy\"), test_data)\n",
    "    \n",
    "    # Print dataset shapes\n",
    "    print(f\"Train data shape: {train_data.shape}\")\n",
    "    print(f\"Validation data shape: {validate_data.shape}\")\n",
    "    print(f\"Test data shape: {test_data.shape}\")\n",
    "    print(f\"Datasets saved in {output_dir} as {dataset_prefix}_train_data.npy, {dataset_prefix}_validate_data.npy, and {dataset_prefix}_test_data.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "547dd30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_saved_data(dataset_name, input_dir=\"./dataset/\"):\n",
    "    \"\"\"\n",
    "    Loads the train, validation, and test datasets from the specified directory using the given dataset name.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataset_name: str, name of the dataset (prefix used when saving)\n",
    "    - input_dir: str, directory containing the saved subsets\n",
    "    \n",
    "    Returns:\n",
    "    - train_data: numpy array\n",
    "    - validate_data: numpy array\n",
    "    - test_data: numpy array\n",
    "    \"\"\"\n",
    "    if not os.path.exists(input_dir):\n",
    "        raise FileNotFoundError(f\"Directory '{input_dir}' does not exist.\")\n",
    "\n",
    "    # Construct file paths using the dataset name\n",
    "    train_data_path = os.path.join(input_dir, f\"{dataset_name}_train_data.npy\")\n",
    "    validate_data_path = os.path.join(input_dir, f\"{dataset_name}_validate_data.npy\")\n",
    "    test_data_path = os.path.join(input_dir, f\"{dataset_name}_test_data.npy\")\n",
    "\n",
    "    # Check if files exist before loading\n",
    "    for file_path in [train_data_path, validate_data_path, test_data_path]:\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "    # Load data\n",
    "    train_data = np.load(train_data_path, allow_pickle=False)\n",
    "    validate_data = np.load(validate_data_path, allow_pickle=False)\n",
    "    test_data = np.load(test_data_path, allow_pickle=False)\n",
    "\n",
    "    # Ensure all loaded data is in dense format\n",
    "    validate_data = np.asarray(validate_data, dtype=np.float64)\n",
    "    test_data = np.asarray(test_data, dtype=np.float64)\n",
    "    \n",
    "    train_data, validate_data, test_data = train_data.T, validate_data.T, test_data.T\n",
    "\n",
    "    # Print dataset shapes\n",
    "    print(f\"Loaded dataset: {dataset_name}\")\n",
    "    print(f\"Train data shape: {train_data.shape}\")\n",
    "    print(f\"Validation data shape: {validate_data.shape}\")\n",
    "    print(f\"Test data shape: {test_data.shape}\")\n",
    "    \n",
    "    return train_data, validate_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbbf2f2",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb8e8134",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_phi_subsets_g(m, g, n, d_thresh=0.4):\n",
    "    \"\"\"\n",
    "    Generates a random measurement matrix (Phi) with m pools and g genes while ensuring minimal correlation.\n",
    "    \n",
    "    Parameters:\n",
    "        m (int): Number of pools (samples).\n",
    "        g (int): Number of genes.\n",
    "        n (tuple): Range (min, max) for the number of pools each gene is assigned to.\n",
    "        d_thresh (float): Maximum allowable correlation threshold between gene assignments.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: A (m x g) matrix where each column represents gene assignment to pools.\n",
    "    \"\"\"\n",
    "    Phi = np.zeros((m, g))  # Initialize measurement matrix\n",
    "    Phi[np.random.choice(m, np.random.randint(n[0], n[1]+1), replace=False), 0] = 1  # Assign first gene\n",
    "    \n",
    "    for i in range(1, g):  # Assign pools for remaining genes\n",
    "        dmax = 1\n",
    "        while dmax > d_thresh:\n",
    "            p = np.zeros(m)\n",
    "            p[np.random.choice(m, np.random.randint(n[0], n[1]+1), replace=False)] = 1  # Randomly assign pools\n",
    "            dmax = 1 - distance.cdist(Phi[:, :i].T, [p], 'correlation').min()  # Ensure minimal correlation\n",
    "        \n",
    "        Phi[:, i] = p  # Assign gene to pools\n",
    "    \n",
    "    Phi = (Phi.T / Phi.sum(1)).T  # Normalize by row sum\n",
    "    return Phi\n",
    "\n",
    "def get_observations(X0, Phi, snr=5, return_noise=False):\n",
    "    \"\"\"\n",
    "    Simulates observations by applying the measurement matrix (Phi) to the true signal (X0) with added noise.\n",
    "    \n",
    "    Parameters:\n",
    "        X0 (np.ndarray): Original gene expression matrix (genes x samples).\n",
    "        Phi (np.ndarray): Measurement matrix mapping genes to pools.\n",
    "        snr (float): Signal-to-noise ratio.\n",
    "        return_noise (bool): If True, return noise separately.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Noisy observed measurements (pools x samples).\n",
    "        (Optional) np.ndarray: Noise matrix.\n",
    "    \"\"\"\n",
    "    noise = np.array([np.random.randn(X0.shape[1]) for _ in range(X0.shape[0])])\n",
    "    noise *= np.linalg.norm(X0) / np.linalg.norm(noise) / snr  # Scale noise level\n",
    "    \n",
    "    if return_noise:\n",
    "        return Phi.dot(X0 + noise), noise\n",
    "    else:\n",
    "        return Phi.dot(X0 + noise)\n",
    "\n",
    "def compare_distances(A, B, random_samples=[], s=200, pvalues=False):\n",
    "    \"\"\"\n",
    "    Compares the Euclidean distances between corresponding columns of matrices A and B.\n",
    "    \n",
    "    Parameters:\n",
    "        A (np.ndarray): First data matrix.\n",
    "        B (np.ndarray): Second data matrix.\n",
    "        random_samples (list, optional): Boolean mask of selected samples.\n",
    "        s (int): Number of samples to randomly select if not provided.\n",
    "        pvalues (bool): Whether to return p-values for the correlation tests.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Pearson and Spearman correlation coefficients (and p-values if pvalues=True).\n",
    "    \"\"\"\n",
    "    if len(random_samples) == 0:\n",
    "        random_samples = np.zeros(A.shape[1], dtype=bool)\n",
    "        random_samples[:min(s, A.shape[1])] = True  # Select `s` random samples\n",
    "        np.random.shuffle(random_samples)\n",
    "    \n",
    "    dist_x = distance.pdist(A[:, random_samples].T, 'euclidean')\n",
    "    dist_y = distance.pdist(B[:, random_samples].T, 'euclidean')\n",
    "    \n",
    "    pear = pearsonr(dist_x, dist_y)  # Pearson correlation\n",
    "    spear = spearmanr(dist_x, dist_y)  # Spearman correlation\n",
    "    \n",
    "    if pvalues:\n",
    "        return pear, spear  # Return correlations with p-values\n",
    "    else:\n",
    "        return pear[0], spear[0]  # Return only correlation coefficients\n",
    "\n",
    "def correlations(A, B):\n",
    "    \"\"\"\n",
    "    Computes correlation metrics between two matrices A and B.\n",
    "    \n",
    "    Parameters:\n",
    "        A (np.ndarray): First matrix.\n",
    "        B (np.ndarray): Second matrix.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Overall correlation, Spearman correlation, per-gene correlation, per-sample correlation.\n",
    "    \"\"\"\n",
    "    p = (1 - distance.correlation(A.flatten(), B.flatten()))  # Overall correlation\n",
    "    spear = spearmanr(A.flatten(), B.flatten())  # Spearman correlation\n",
    "    \n",
    "    dist_genes = np.zeros(A.shape[0])  # Per-gene correlation\n",
    "    for i in range(A.shape[0]):\n",
    "        dist_genes[i] = 1 - distance.correlation(A[i], B[i])\n",
    "    \n",
    "    pg = np.average(dist_genes[np.isfinite(dist_genes)])  # Mean per-gene correlation\n",
    "    \n",
    "    dist_sample = np.zeros(A.shape[1])  # Per-sample correlation\n",
    "    for i in range(A.shape[1]):\n",
    "        dist_sample[i] = 1 - distance.correlation(A[:, i], B[:, i])\n",
    "    \n",
    "    ps = np.average(dist_sample[np.isfinite(dist_sample)])  # Mean per-sample correlation\n",
    "    \n",
    "    return p, spear[0], pg, ps\n",
    "\n",
    "def compare_results(A, B):\n",
    "    \"\"\"\n",
    "    Aggregates multiple comparison metrics between matrices A and B.\n",
    "    \n",
    "    Parameters:\n",
    "        A (np.ndarray): First matrix.\n",
    "        B (np.ndarray): Second matrix.\n",
    "    \n",
    "    Returns:\n",
    "        list: Combined correlation and distance comparison results.\n",
    "    \"\"\"\n",
    "    results = list(correlations(A, B))[:-1]  # Get correlation metrics (excluding `ps`)\n",
    "    results += list(compare_distances(A, B))  # Compare Euclidean distances (gene-wise)\n",
    "    results += list(compare_distances(A.T, B.T))  # Compare Euclidean distances (sample-wise)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b35639",
   "metadata": {},
   "source": [
    "## Sparse Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "181be75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparse_decode(Y, D, k, worstFit=1., mink=0, method='omp', nonneg=False):\n",
    "    \"\"\"\n",
    "    Sparse decoding method - obtain module activations from composite measurements.\n",
    "    \n",
    "    Parameters:\n",
    "    - Y : Observed measurement data (pools × samples)\n",
    "    - D : Dictionary (module patterns) used for reconstruction\n",
    "    - k : Sparsity constraint (number of nonzero coefficients)\n",
    "    - worstFit : Minimum required reconstruction accuracy (default = 1)\n",
    "    - mink : Minimum sparsity allowed (default = 0)\n",
    "    - method : 'omp' (Orthogonal Matching Pursuit) or 'lasso' (L1 regularization)\n",
    "    - nonneg : If True, forces non-negative solutions (for LASSO only)\n",
    "    \n",
    "    Returns:\n",
    "    - W : Estimated module activations (modules × samples)\n",
    "    \"\"\"\n",
    "    \n",
    "    if method == 'omp':\n",
    "        while k > mink:\n",
    "            W = spams.omp(np.asfortranarray(Y), np.asfortranarray(D), L=k, numThreads=THREADS)\n",
    "            W = np.asarray(W.todense())\n",
    "\n",
    "            fit = 1 - np.linalg.norm(Y - D.dot(W))**2 / np.linalg.norm(Y)**2  # Compute fit accuracy\n",
    "\n",
    "            if fit < worstFit:\n",
    "                break  # Stop if fit is too low\n",
    "            else:\n",
    "                k -= 1  # Decrease sparsity constraint\n",
    "                \n",
    "            \n",
    "    elif method == 'lasso':\n",
    "        Ynorm = np.linalg.norm(Y)**2 / Y.shape[1]  # Normalize Y\n",
    "        W = spams.lasso(np.asfortranarray(Y), np.asfortranarray(D),\n",
    "                        lambda1=k * Ynorm, mode=1, numThreads=THREADS, pos=nonneg)\n",
    "        W = np.asarray(W.todense())\n",
    "    \n",
    "    return W"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e201ca8",
   "metadata": {},
   "source": [
    "## Modules Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8b0a7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smaf(X, d, lda1, lda2, maxItr=10, UW=None, posW=False, posU=True, use_chol=False, \n",
    "         module_lower=1, activity_lower=1, donorm=False, mode=1, mink=5, U0=[], \n",
    "         U0_delta=0.1, doprint=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Sparse Module Activity Factorization (SMAF) to learn gene modules.\n",
    "    \n",
    "    Parameters:\n",
    "    - X: np.array, shape (genes, cells), gene expression matrix. Note: dims opposite of anndata \n",
    "    - d: int, number of gene modules (dictionary size)\n",
    "    - lda1: float, regularization term for activity sparsity\n",
    "    - lda2: float, regularization term for module sparsity\n",
    "    - maxItr: int, number of iterations for optimization\n",
    "    - UW: tuple, (U, W) initial matrices, optional\n",
    "    - posW: bool, enforce non-negativity on W\n",
    "    - posU: bool, enforce non-negativity on U\n",
    "    - use_chol: bool, use Cholesky decomposition for faster computation\n",
    "    - module_lower: float, lower bound for module entropy\n",
    "    - activity_lower: float, lower bound for activity entropy\n",
    "    - donorm: bool, normalize U matrix\n",
    "    - mode: int, optimization mode\n",
    "    - mink: int, minimum sparsity constraint\n",
    "    - U0: list, optional initialization for U\n",
    "    - U0_delta: float, delta for projected gradient descent\n",
    "    - doprint: bool, print progress\n",
    "    \n",
    "    Returns:\n",
    "    - U, W: np.arrays, learned dictionary and activity matrices\n",
    "    \"\"\"\n",
    "    # Initialize U and W matrices if not provided\n",
    "    if UW is None:\n",
    "        U, W = spams.nmf(np.asfortranarray(X), return_lasso=True, K=d, numThreads=THREADS)\n",
    "        W = np.asarray(W.todense())  # Convert sparse W to dense format\n",
    "    else:\n",
    "        U, W = UW  # Use provided matrices\n",
    "    \n",
    "    # Compute initial reconstruction of X\n",
    "    Xhat = U.dot(W)\n",
    "    # Compute normalization factor for regularization\n",
    "    Xnorm = np.linalg.norm(X) ** 2 / X.shape[1]\n",
    "    \n",
    "    # Iterate to optimize U and W\n",
    "    for itr in range(maxItr):\n",
    "        if mode == 1:\n",
    "            # Solve for U using Lasso regression with sparsity regularization\n",
    "            U = spams.lasso(np.asfortranarray(X.T), D=np.asfortranarray(W.T),\n",
    "                            lambda1=lda2 * Xnorm, mode=1, numThreads=THREADS,\n",
    "                            cholesky=use_chol, pos=posU)\n",
    "            U = np.asarray(U.todense()).T  # Convert to dense and transpose\n",
    "        elif mode == 2:\n",
    "            # Optionally use projected gradient descent if U0 is provided\n",
    "            if len(U0) > 0:\n",
    "                U = projected_grad_desc(W.T, X.T, U.T, U0.T, lda2, U0_delta, maxItr=400)\n",
    "                U = U.T  # Transpose back\n",
    "            else:\n",
    "                # Solve for U using Lasso regression with different lambda settings\n",
    "                U = spams.lasso(np.asfortranarray(X.T), D=np.asfortranarray(W.T),\n",
    "                                lambda1=lda2, lambda2=0.0, mode=2, numThreads=THREADS,\n",
    "                                cholesky=use_chol, pos=posU)\n",
    "                U = np.asarray(U.todense()).T  # Convert to dense and transpose\n",
    "        \n",
    "        # Normalize U if required\n",
    "        if donorm:\n",
    "            U = U / np.linalg.norm(U, axis=0)\n",
    "            U[np.isnan(U)] = 0  # Replace NaN values with zero\n",
    "        \n",
    "        # Solve for W\n",
    "        if mode == 1:\n",
    "            wf = (1 - lda2)  # Worst-fit tolerance for sparsity\n",
    "            W = sparse_decode(X, U, lda1, worstFit=wf, mink=mink)\n",
    "        elif mode == 2:\n",
    "            if len(U0) > 0:\n",
    "                W = projected_grad_desc(U, X, W, [], lda1, 0., nonneg=posW, maxItr=400)\n",
    "            else:\n",
    "                W = spams.lasso(np.asfortranarray(X), D=np.asfortranarray(U),\n",
    "                                lambda1=lda1, lambda2=1.0, mode=2, numThreads=THREADS,\n",
    "                                cholesky=use_chol, pos=posW)\n",
    "                W = np.asarray(W.todense())  # Convert to dense\n",
    "        \n",
    "        # Compute updated reconstruction of X\n",
    "        Xhat = U.dot(W)\n",
    "        \n",
    "        # Compute module and activity sizes based on entropy\n",
    "        module_size = np.average([np.exp(entropy(abs(u))) for u in U.T if u.sum() > 0])\n",
    "        activity_size = np.average([np.exp(entropy(abs(w))) for w in W.T])\n",
    "        \n",
    "        # Print progress if required\n",
    "        if doprint:\n",
    "            print(distance.correlation(X.flatten(), Xhat.flatten()), module_size, activity_size, lda1, lda2)\n",
    "        \n",
    "        # Adjust sparsity parameters dynamically\n",
    "        if module_size < module_lower:\n",
    "            lda2 /= 2.  # Decrease sparsity regularization for U\n",
    "        if activity_size < activity_lower:\n",
    "            lda2 /= 2.  # Decrease sparsity regularization for W\n",
    "    \n",
    "    return U, W  # Return learned matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868d92ea",
   "metadata": {},
   "source": [
    "## Random Double Balanced Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35276d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_double_balanced(m, g, max_pools_per_gene, min_pools_per_gene):\n",
    "    \"\"\"\n",
    "    Generates a random measurement matrix (`phi`) with \"double balanced\" characteristics.\n",
    "    \n",
    "    This function ensures:\n",
    "    - Each gene is assigned to `max_pools_per_gene` pools initially.\n",
    "    - Genes with fewer than `min_pools_per_gene` assignments are reassigned.\n",
    "    - The resulting matrix is normalized so that each row sums to 1.\n",
    "\n",
    "    Parameters:\n",
    "    - m : int\n",
    "        Number of pools (rows in the matrix).\n",
    "    - g : int\n",
    "        Number of genes (columns in the matrix).\n",
    "    - max_pools_per_gene : int\n",
    "        Maximum number of pools a gene can be assigned to.\n",
    "    - min_pools_per_gene : int\n",
    "        Minimum number of pools a gene must be assigned to.\n",
    "\n",
    "    Returns:\n",
    "    - phi : np.ndarray\n",
    "        A (m × g) measurement matrix where each gene is assigned to a set of pools.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize an empty measurement matrix (pools × genes)\n",
    "    phi = np.zeros((m, g))\n",
    "\n",
    "    # Randomly assign each gene to pools up to `max_pools_per_gene` times\n",
    "    for i in range(max_pools_per_gene):\n",
    "        idx = np.random.choice(g, g, replace=False)  # Shuffle gene indices\n",
    "        idx = idx % m  # Ensure indices fit within the pool size (modulo operation)\n",
    "        phi[idx, np.arange(g)] = 1  # Assign genes to random pools\n",
    "\n",
    "    # Ensure each gene is assigned to at least `min_pools_per_gene` pools\n",
    "    for i in np.where(phi.sum(0) < min_pools_per_gene)[0]:  # Identify under-assigned genes\n",
    "        p = phi.sum(1).max() - phi.sum(1)  # Compute imbalance for each pool\n",
    "        p[np.where(phi[:, i])[0]] = 0  # Exclude pools that already contain the gene\n",
    "\n",
    "        # If there are pools available for reassignment\n",
    "        if p.sum() > 0:\n",
    "            p = p / p.sum()  # Normalize probability distribution\n",
    "            num_to_assign = min((p > 0).sum(), int(min_pools_per_gene - phi[:, i].sum()))\n",
    "            idx = np.random.choice(m, num_to_assign, replace=False, p=p)  # Select new pools\n",
    "            phi[idx, i] = 1  # Assign gene to additional pools\n",
    "\n",
    "    # Normalize the matrix so that each row sums to 1 (avoid bias in pooling)\n",
    "    phi = (phi.T / phi.sum(1)).T\n",
    "\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f57862d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_coherence_matrices(m, g, max_pools_per_gene, min_pools_per_gene, U, num_matrices=5000, num_best=500):\n",
    "    \"\"\"\n",
    "    Generates a set of random measurement matrices and selects the `num_best` matrices\n",
    "    with the lowest (best) 90th percentile coherence scores.\n",
    "\n",
    "    Parameters:\n",
    "        num_matrices (int): Total number of random matrices to generate.\n",
    "        num_best (int): Number of best matrices to retain.\n",
    "        num_pools (int): Number of pools (rows in the measurement matrix).\n",
    "        num_features (int): Number of features (columns in the measurement matrix).\n",
    "        num_min (int): Minimum number of pools assigned per feature.\n",
    "        num_max (int): Maximum number of pools assigned per feature.\n",
    "        U (np.ndarray): Feature transformation matrix for coherence computation.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (best coherence scores, corresponding best measurement matrices)\n",
    "    \"\"\"\n",
    "    best = np.ones(num_best)  # Initialize best coherence scores (worst possible score = 1)\n",
    "    Phi_coh = [None for _ in best]  # Corresponding best measurement matrices\n",
    "\n",
    "    for x in range(num_matrices):  \n",
    "        if np.mod(x, num_matrices // 10) == 0:  # Print progress every 10% of iterations\n",
    "            print(f\"Iteration {x} of {num_matrices}\")\n",
    "\n",
    "        # Generate a new random measurement matrix\n",
    "        phi = random_double_balanced(m, g, max_pools_per_gene, min_pools_per_gene)  \n",
    "\n",
    "        # Compute the 90th percentile of the cosine distance between projected feature vectors\n",
    "        coh_90 = np.percentile(1 - distance.pdist(phi.dot(U).T, 'cosine'), 90)\n",
    "\n",
    "        # If the new `phi` has a better coherence score, replace the worst-performing matrix\n",
    "        if coh_90 < best.max():  \n",
    "            i = np.argmax(best)  # Find the index of the worst current coherence score\n",
    "            best[i] = coh_90  # Replace the worst score with the new, better coherence score\n",
    "            Phi_coh[i] = phi  # Store the corresponding measurement matrix\n",
    "    \n",
    "    return best, Phi_coh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89873054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_reconstruction_matrices(Phi_coh, validate_data, U, sparsity=0.02, num_best=50):\n",
    "    \"\"\"\n",
    "    Selects the best measurement matrices based on their ability to recover original gene expression patterns.\n",
    "\n",
    "    Parameters:\n",
    "        Phi_coh (list): List of best measurement matrices from coherence selection.\n",
    "        validate_data (np.ndarray): Original validation dataset.\n",
    "        U (np.ndarray): Feature transformation matrix.\n",
    "        sparsity (float): Sparsity constraint for sparse decoding (default: 0.02).\n",
    "        num_best (int): Number of best matrices to retain.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (best reconstruction scores, corresponding best measurement matrices)\n",
    "    \"\"\"\n",
    "    best = np.zeros(num_best)  # Initialize best reconstruction scores (worst possible score = 0)\n",
    "    Phi = [None for _ in best]  # Corresponding best measurement matrices\n",
    "\n",
    "    for phi in Phi_coh:\n",
    "        # Generate simulated observations by applying `phi` to the validation data\n",
    "        y = get_observations(validate_data, phi, snr=5)  # Simulate pooled noisy measurements\n",
    "        \n",
    "        # Use sparse decoding (LASSO) to recover gene module activations\n",
    "        w = sparse_decode(y, phi.dot(U), sparsity, method='lasso')  \n",
    "        \n",
    "        # Reconstruct gene expression using the estimated module activations\n",
    "        x2 = U.dot(w)  # Approximate gene expression matrix from recovered module weights\n",
    "        \n",
    "        # Compare reconstructed expression (`x2`) with the original validation data (`validate_data`)\n",
    "        r = compare_results(validate_data, x2)  \n",
    "        \n",
    "        # If the new measurement matrix `phi` produces a better reconstruction, update `best` and `Phi`\n",
    "        if r[2] > best.min():  # If the new reconstruction score is better than the worst stored score\n",
    "            i = np.argmin(best)  # Find the index of the worst-performing matrix\n",
    "            best[i] = r[2]  # Replace the worst reconstruction score with the new, better score\n",
    "            Phi[i] = phi  # Store the corresponding measurement matrix\n",
    "    \n",
    "    return best, Phi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffb98d7",
   "metadata": {},
   "source": [
    "# Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "817bacd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_simulation(\n",
    "    adata_path, \n",
    "    gene_set_size,\n",
    "    num_cells, \n",
    "    num_measurements, \n",
    "    min_pools_per_gene, \n",
    "    max_pools_per_gene, \n",
    "    sparsity, \n",
    "    num_modules,\n",
    "    dataset_dir=\"./dataset/\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs the full simulation pipeline to optimize measurement matrices for gene expression analysis.\n",
    "    \n",
    "    Parameters:\n",
    "        adata_path (str): Path to the AnnData file.\n",
    "        gene_set_size (int): Number of genes to include in the simulation (500, 1000, or 5000).\n",
    "        num_cells (int): Number of cells to include in the simulation.\n",
    "        num_measurements (int): Number of measurement pools.\n",
    "        min_pools_per_gene (int): Minimum pools a gene must be assigned to.\n",
    "        max_pools_per_gene (int): Maximum pools a gene can be assigned to.\n",
    "        sparsity (float): Sparsity constraint for sparse decoding.\n",
    "        num_modules (int): Number of gene modules (dictionary size in SMAF).\n",
    "        dataset_dir (str): Directory where the dataset is stored.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Best measurement matrix and evaluation metrics.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the AnnData object\n",
    "    adata = sc.read_h5ad(adata_path)\n",
    "    \n",
    "    # Downsample the AnnData object\n",
    "    adata_subset = downsample_adata(adata, gene_set_size, num_cells, output_path=dataset_dir)\n",
    "    \n",
    "    # Split the dataset into train, validation, and test sets\n",
    "    split_and_save_data(adata_subset, \"subset_data\", dataset_dir)\n",
    "    \n",
    "    # Load the split data\n",
    "    train_data, validate_data, test_data = load_saved_data(\"subset_data\", dataset_dir)\n",
    "    \n",
    "    # Generate gene modules matrix U using SMAF\n",
    "    U, W = smaf(train_data, num_modules, lda1=8, lda2=0.2, maxItr=20,\n",
    "                use_chol=False, donorm=True, mode=1, mink=0., doprint=True)\n",
    "    \n",
    "    # Remove zero-contribution modules\n",
    "    nz = (U.sum(axis=0) > 0)\n",
    "    U = U[:, nz]\n",
    "    \n",
    "    print(\"U dimentions =\", U.shape)\n",
    "    \n",
    "    # Generate and evaluate measurement matrices based on coherence\n",
    "    best_coh_scores, Phi_coh = find_best_coherence_matrices(m=num_measurements, g=train_data.shape[0], \n",
    "        min_pools_per_gene=min_pools_per_gene, max_pools_per_gene=max_pools_per_gene, U=U,\n",
    "                                                            num_matrices=5000, num_best=500,\n",
    "    )\n",
    "    \n",
    "    # Find best measurement matrices based on reconstruction quality\n",
    "    best_rec_scores, Phi_best = find_best_reconstruction_matrices(\n",
    "        Phi_coh, validate_data, U, sparsity, num_best=50\n",
    "    )\n",
    "    \n",
    "    # Select the best matrix based on reconstruction quality\n",
    "    best_idx = np.argmax(best_rec_scores)\n",
    "    best_matrix = Phi_best[best_idx]\n",
    "    best_score = best_rec_scores[best_idx]\n",
    "    \n",
    "    return {\n",
    "        \"best_matrix\": best_matrix,\n",
    "        \"best_score\": best_score,\n",
    "        \"best_coherence_scores\": best_coh_scores,\n",
    "        \"best_reconstruction_scores\": best_rec_scores\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8184ee",
   "metadata": {},
   "source": [
    "# Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c4e83c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched genes for 1000 set: 892\n",
      "Downsampled dataset saved to ./dataset/cerebellum/cb_mouse_1000genes.h5ad\n",
      "Train data shape: (5000, 892)\n",
      "Validation data shape: (2500, 892)\n",
      "Test data shape: (2500, 892)\n",
      "Datasets saved in ./dataset/cerebellum/cb_mouse as subset_data_train_data.npy, subset_data_validate_data.npy, and subset_data_test_data.npy\n",
      "Loaded dataset: subset_data\n",
      "Train data shape: (892, 5000)\n",
      "Validation data shape: (892, 2500)\n",
      "Test data shape: (892, 2500)\n",
      "n_genes = 892\n",
      "Iteration 0 of 5000\n",
      "Iteration 500 of 5000\n",
      "Iteration 1000 of 5000\n",
      "Iteration 1500 of 5000\n",
      "Iteration 2000 of 5000\n",
      "Iteration 2500 of 5000\n",
      "Iteration 3000 of 5000\n",
      "Iteration 3500 of 5000\n",
      "Iteration 4000 of 5000\n",
      "Iteration 4500 of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projectnb/bioinfor/myousry/miniconda3/envs/comprsense/lib/python3.12/site-packages/scipy/spatial/distance.py:685: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  dist = 1.0 - uv / math.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Measurement Matrix Shape: (50, 892)\n",
      "Best Score: 0.6423503633716635\n",
      "Top 5 Coherence Scores: [0.81664043 0.81957561 0.81482166 0.81603927 0.81343413]\n",
      "Top 5 Reconstruction Scores: [0.6383733  0.63653413 0.63630183 0.63523376 0.63525676]\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "adata_path = \"dataset/cerebellum/cb_adult_mouse.h5ad\"  # Path to your AnnData file\n",
    "gene_set_size = 1000  # Choose from 500, 1000, or 5000\n",
    "num_cells = 10000  # Number of cells to include in the simulation\n",
    "num_measurements = 50  # Number of measurement pools\n",
    "min_pools_per_gene = 4  # Minimum number of pools per gene\n",
    "max_pools_per_gene = 4  # Maximum number of pools per gene\n",
    "sparsity = 0.02  # Sparsity constraint for sparse decoding\n",
    "num_modules = 50  # Number of gene modules\n",
    "dataset_dir = \"./dataset/cerebellum/cb_mouse\"  # Directory where datasets are stored\n",
    "\n",
    "# Run the simulation\n",
    "results = run_simulation(\n",
    "    adata_path=adata_path,\n",
    "    gene_set_size=gene_set_size,\n",
    "    num_cells=num_cells,\n",
    "    num_measurements=num_measurements,\n",
    "    min_pools_per_gene=min_pools_per_gene,\n",
    "    max_pools_per_gene=max_pools_per_gene,\n",
    "    sparsity=sparsity,\n",
    "    num_modules=num_modules,\n",
    "    dataset_dir=dataset_dir\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"Best Measurement Matrix Shape:\", results[\"best_matrix\"].shape)\n",
    "print(\"Best Score:\", results[\"best_score\"])\n",
    "print(\"Top 5 Coherence Scores:\", results[\"best_coherence_scores\"][:5])\n",
    "print(\"Top 5 Reconstruction Scores:\", results[\"best_reconstruction_scores\"][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7bf5dc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched genes for 500 set: 444\n",
      "Downsampled dataset saved to ./dataset/cerebellum/cb_mouse_500_500genes.h5ad\n",
      "Train data shape: (5000, 444)\n",
      "Validation data shape: (2500, 444)\n",
      "Test data shape: (2500, 444)\n",
      "Datasets saved in ./dataset/cerebellum/cb_mouse_500 as subset_data_train_data.npy, subset_data_validate_data.npy, and subset_data_test_data.npy\n",
      "Loaded dataset: subset_data\n",
      "Train data shape: (444, 5000)\n",
      "Validation data shape: (444, 2500)\n",
      "Test data shape: (444, 2500)\n",
      "n_genes = 444\n",
      "Iteration 0 of 5000\n",
      "Iteration 500 of 5000\n",
      "Iteration 1000 of 5000\n",
      "Iteration 1500 of 5000\n",
      "Iteration 2000 of 5000\n",
      "Iteration 2500 of 5000\n",
      "Iteration 3000 of 5000\n",
      "Iteration 3500 of 5000\n",
      "Iteration 4000 of 5000\n",
      "Iteration 4500 of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projectnb/bioinfor/myousry/miniconda3/envs/comprsense/lib/python3.12/site-packages/scipy/spatial/distance.py:685: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  dist = 1.0 - uv / math.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Measurement Matrix Shape: (50, 444)\n",
      "Best Score: 0.3605855130943314\n",
      "Top 5 Coherence Scores: [0.83021409 0.84573697 0.84569205 0.84427742 0.85104683]\n",
      "Top 5 Reconstruction Scores: [0.35449873 0.35638015 0.35613819 0.35691696 0.35438521]\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "adata_path = \"dataset/cerebellum/cb_adult_mouse.h5ad\"  # Path to your AnnData file\n",
    "gene_set_size = 500  # Choose from 500, 1000, or 5000\n",
    "num_cells = 10000  # Number of cells to include in the simulation\n",
    "num_measurements = 50  # Number of measurement pools\n",
    "min_pools_per_gene = 4  # Minimum number of pools per gene\n",
    "max_pools_per_gene = 4  # Maximum number of pools per gene\n",
    "sparsity = 0.02  # Sparsity constraint for sparse decoding\n",
    "num_modules = 50  # Number of gene modules\n",
    "dataset_dir = \"./dataset/cerebellum/cb_mouse_500\"  # Directory where datasets are stored\n",
    "\n",
    "# Run the simulation\n",
    "results = run_simulation(\n",
    "    adata_path=adata_path,\n",
    "    gene_set_size=gene_set_size,\n",
    "    num_cells=num_cells,\n",
    "    num_measurements=num_measurements,\n",
    "    min_pools_per_gene=min_pools_per_gene,\n",
    "    max_pools_per_gene=max_pools_per_gene,\n",
    "    sparsity=sparsity,\n",
    "    num_modules=num_modules,\n",
    "    dataset_dir=dataset_dir\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"Best Measurement Matrix Shape:\", results[\"best_matrix\"].shape)\n",
    "print(\"Best Score:\", results[\"best_score\"])\n",
    "print(\"Top 5 Coherence Scores:\", results[\"best_coherence_scores\"][:5])\n",
    "print(\"Top 5 Reconstruction Scores:\", results[\"best_reconstruction_scores\"][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9d4ff1f8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched genes for 1000 set: 892\n",
      "Downsampled dataset saved to ./dataset/cerebellum_1000genes.h5ad\n",
      "Train data shape: (5000, 892)\n",
      "Validation data shape: (2500, 892)\n",
      "Test data shape: (2500, 892)\n",
      "Datasets saved in ./dataset/cerebellum as subset_data_train_data.npy, subset_data_validate_data.npy, and subset_data_test_data.npy\n",
      "Loaded dataset: subset_data\n",
      "Train data shape: (892, 5000)\n",
      "Validation data shape: (892, 2500)\n",
      "Test data shape: (892, 2500)\n",
      "n_genes = 892\n",
      "Iteration 0 of 5000\n",
      "Iteration 500 of 5000\n",
      "Iteration 1000 of 5000\n",
      "Iteration 1500 of 5000\n",
      "Iteration 2000 of 5000\n",
      "Iteration 2500 of 5000\n",
      "Iteration 3000 of 5000\n",
      "Iteration 3500 of 5000\n",
      "Iteration 4000 of 5000\n",
      "Iteration 4500 of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projectnb/bioinfor/myousry/miniconda3/envs/comprsense/lib/python3.12/site-packages/scipy/spatial/distance.py:685: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  dist = 1.0 - uv / math.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean recovery score for all 1000 genes: 0.5261\n",
      "Mean recovery score for the 500 genes (from 1000-gene simulation): 0.5126\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc  # For AnnData\n",
    "\n",
    "def run_simulation(\n",
    "    adata_path, \n",
    "    gene_set_size,\n",
    "    num_cells, \n",
    "    num_measurements, \n",
    "    min_pools_per_gene, \n",
    "    max_pools_per_gene, \n",
    "    sparsity, \n",
    "    num_modules,\n",
    "    dataset_dir=\"./dataset/\",\n",
    "    subset_gene_indices=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs the full simulation pipeline to optimize measurement matrices for gene expression analysis.\n",
    "    Now correctly extracts subset gene reconstruction scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the AnnData object\n",
    "    adata = sc.read_h5ad(adata_path)\n",
    "    \n",
    "    # Downsample the AnnData object\n",
    "    adata_subset = downsample_adata(adata, gene_set_size, num_cells, output_path=dataset_dir)\n",
    "    \n",
    "    # Split the dataset into train, validation, and test sets\n",
    "    split_and_save_data(adata_subset, \"subset_data\", dataset_dir)\n",
    "    \n",
    "    # Load the split data\n",
    "    train_data, validate_data, test_data = load_saved_data(\"subset_data\", dataset_dir)\n",
    "    \n",
    "    # Convert validation data to correct format (Fixes SPAMS error)\n",
    "    validate_data = np.asfortranarray(validate_data.astype(np.float64))\n",
    "    \n",
    "    # Generate gene modules matrix U using SMAF\n",
    "    U, W = smaf(train_data, num_modules, lda1=8, lda2=0.2, maxItr=100)\n",
    "    \n",
    "    # Remove zero-contribution modules\n",
    "    nz = (U.sum(axis=0) > 0)\n",
    "    U = U[:, nz]\n",
    "    \n",
    "    print(\"n_genes =\", train_data.shape[0])\n",
    "    \n",
    "    # Generate and evaluate measurement matrices based on coherence\n",
    "    best_coh_scores, Phi_coh = find_best_coherence_matrices(m=num_measurements, g=train_data.shape[0], \n",
    "        min_pools_per_gene=min_pools_per_gene, max_pools_per_gene=max_pools_per_gene, U=U,\n",
    "        num_matrices=5000, num_best=500\n",
    "    )\n",
    "    \n",
    "    # Find best measurement matrices based on reconstruction quality\n",
    "    best_rec_scores_all, best_rec_scores_subset, Phi_best = find_best_reconstruction_matrices(\n",
    "        Phi_coh, validate_data, U, sparsity, num_best=50, subset_gene_indices=subset_gene_indices\n",
    "    )\n",
    "\n",
    "    # Select the best matrix based on reconstruction quality\n",
    "    best_idx = np.argmax(best_rec_scores_all)\n",
    "    best_matrix = Phi_best[best_idx]\n",
    "    best_score = best_rec_scores_all[best_idx]\n",
    "    \n",
    "    return {\n",
    "        \"best_matrix\": best_matrix,\n",
    "        \"best_score\": best_score,\n",
    "        \"best_coherence_scores\": best_coh_scores,\n",
    "        \"best_reconstruction_scores_all\": best_rec_scores_all,\n",
    "        \"best_reconstruction_scores_subset\": best_rec_scores_subset\n",
    "    }\n",
    "\n",
    "\n",
    "def find_best_reconstruction_matrices(\n",
    "    Phi_coh, validate_data, U, sparsity=0.02, num_best=50, subset_gene_indices=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Selects the best measurement matrices based on their ability to recover original gene expression patterns.\n",
    "    Now correctly extracts the third value from `compare_results()`, which is the reconstruction score.\n",
    "    \"\"\"\n",
    "    best_all = np.zeros(num_best)  \n",
    "    best_subset = np.zeros(num_best) if subset_gene_indices is not None else None\n",
    "    Phi = [None for _ in best_all]  \n",
    "\n",
    "    for i, phi in enumerate(Phi_coh):\n",
    "        y = get_observations(validate_data, phi, snr=5)\n",
    "        w = sparse_decode(y, phi.dot(U), sparsity, method='lasso')\n",
    "        x2 = U.dot(w)  \n",
    "\n",
    "        # Extract reconstruction score correctly\n",
    "        r = compare_results(validate_data, x2)\n",
    "        rec_score = r[2]  # Extract the reconstruction score\n",
    "\n",
    "        subset_score = None\n",
    "        if subset_gene_indices is not None:\n",
    "            subset_reconstruction = x2[subset_gene_indices, :]\n",
    "            subset_original = validate_data[subset_gene_indices, :]\n",
    "            subset_score = compare_results(subset_original, subset_reconstruction)[2]  # Extract score\n",
    "\n",
    "        if rec_score > best_all.min():\n",
    "            idx = np.argmin(best_all)\n",
    "            best_all[idx] = rec_score  \n",
    "            Phi[idx] = phi  \n",
    "            if subset_score is not None:\n",
    "                best_subset[idx] = subset_score  \n",
    "\n",
    "    return best_all, best_subset, Phi\n",
    "\n",
    "\n",
    "# ========== Load Gene Lists and Identify Subset Indices ==========\n",
    "genes_500 = pd.read_csv(\"./dataset/genes_500.csv\", header=None, skiprows=1)[0].tolist()\n",
    "genes_1000 = pd.read_csv(\"./dataset/genes_1000.csv\", header=None, skiprows=1)[0].tolist()\n",
    "\n",
    "# Find indices of genes_500 in genes_1000\n",
    "indices_500_in_1000 = [i for i, gene in enumerate(genes_1000) if gene in genes_500]\n",
    "\n",
    "# ========== Run Simulation for the 1000-Gene Set ==========\n",
    "simulation_results = run_simulation(\n",
    "    adata_path=\"./dataset/cerebellum/cb_mouse_1000genes.h5ad\", \n",
    "    gene_set_size=1000,\n",
    "    num_cells=10000, \n",
    "    num_measurements=50, \n",
    "    min_pools_per_gene=4, \n",
    "    max_pools_per_gene=4, \n",
    "    sparsity=0.02, \n",
    "    num_modules=50,\n",
    "    dataset_dir=\"./dataset/cerebellum\",\n",
    "    subset_gene_indices=indices_500_in_1000  # Pass the subset indices\n",
    ")\n",
    "\n",
    "# Extract reconstruction scores\n",
    "best_rec_scores_all = simulation_results[\"best_reconstruction_scores_all\"]\n",
    "best_rec_scores_subset = simulation_results[\"best_reconstruction_scores_subset\"]\n",
    "\n",
    "# Compute means\n",
    "mean_full_score = np.mean(best_rec_scores_all)\n",
    "mean_subset_score = np.mean(best_rec_scores_subset) if best_rec_scores_subset is not None else None\n",
    "\n",
    "# ========== Print Results ==========\n",
    "print(f\"Mean recovery score for all 1000 genes: {mean_full_score:.4f}\")\n",
    "if mean_subset_score is not None:\n",
    "    print(f\"Mean recovery score for the 500 genes (from 1000-gene simulation): {mean_subset_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4b43d845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched genes for 1000 set: 892\n",
      "Downsampled dataset saved to ./dataset/cerebellum_1000genes.h5ad\n",
      "Train data shape: (5000, 892)\n",
      "Validation data shape: (2500, 892)\n",
      "Test data shape: (2500, 892)\n",
      "Datasets saved in ./dataset/cerebellum as subset_data_train_data.npy, subset_data_validate_data.npy, and subset_data_test_data.npy\n",
      "Loaded dataset: subset_data\n",
      "Train data shape: (892, 5000)\n",
      "Validation data shape: (892, 2500)\n",
      "Test data shape: (892, 2500)\n",
      "n_genes = 892\n",
      "Iteration 0 of 5000\n",
      "Iteration 500 of 5000\n",
      "Iteration 1000 of 5000\n",
      "Iteration 1500 of 5000\n",
      "Iteration 2000 of 5000\n",
      "Iteration 2500 of 5000\n",
      "Iteration 3000 of 5000\n",
      "Iteration 3500 of 5000\n",
      "Iteration 4000 of 5000\n",
      "Iteration 4500 of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projectnb/bioinfor/myousry/miniconda3/envs/comprsense/lib/python3.12/site-packages/scipy/spatial/distance.py:685: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  dist = 1.0 - uv / math.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean recovery score for all 1000 genes: 0.6124\n",
      "Mean recovery score for the 500 genes (from 1000-gene simulation): 0.6318\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scanpy as sc  # For AnnData\n",
    "\n",
    "def run_simulation(\n",
    "    adata_path, \n",
    "    gene_set_size,\n",
    "    num_cells, \n",
    "    num_measurements, \n",
    "    min_pools_per_gene, \n",
    "    max_pools_per_gene, \n",
    "    sparsity, \n",
    "    num_modules,\n",
    "    dataset_dir=\"./dataset/\",\n",
    "    subset_gene_indices=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Runs the full simulation pipeline to optimize measurement matrices for gene expression analysis.\n",
    "    Now correctly extracts subset gene reconstruction scores.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the AnnData object\n",
    "    adata = sc.read_h5ad(adata_path)\n",
    "    \n",
    "    # Downsample the AnnData object\n",
    "    adata_subset = downsample_adata(adata, gene_set_size, num_cells, output_path=dataset_dir)\n",
    "    \n",
    "    # Split the dataset into train, validation, and test sets\n",
    "    split_and_save_data(adata_subset, \"subset_data\", dataset_dir)\n",
    "    \n",
    "    # Load the split data\n",
    "    train_data, validate_data, test_data = load_saved_data(\"subset_data\", dataset_dir)\n",
    "    \n",
    "    # Convert validation data to correct format (Fixes SPAMS error)\n",
    "    validate_data = np.asfortranarray(validate_data.astype(np.float64))\n",
    "    \n",
    "    # Generate gene modules matrix U using SMAF\n",
    "    U, W = smaf(train_data, num_modules, lda1=8, lda2=0.2, maxItr=100)\n",
    "    \n",
    "    # Remove zero-contribution modules\n",
    "    nz = (U.sum(axis=0) > 0)\n",
    "    U = U[:, nz]\n",
    "    \n",
    "    print(\"n_genes =\", train_data.shape[0])\n",
    "    \n",
    "    # Generate and evaluate measurement matrices based on coherence\n",
    "    best_coh_scores, Phi_coh = find_best_coherence_matrices(m=num_measurements, g=train_data.shape[0], \n",
    "        min_pools_per_gene=min_pools_per_gene, max_pools_per_gene=max_pools_per_gene, U=U,\n",
    "        num_matrices=5000, num_best=500\n",
    "    )\n",
    "    \n",
    "    # Find best measurement matrices based on reconstruction quality\n",
    "    best_rec_scores_all, best_rec_scores_subset, Phi_best = find_best_reconstruction_matrices(\n",
    "        Phi_coh, validate_data, U, sparsity, num_best=50, subset_gene_indices=subset_gene_indices\n",
    "    )\n",
    "\n",
    "    # Select the best matrix based on reconstruction quality\n",
    "    best_idx = np.argmax(best_rec_scores_all)\n",
    "    best_matrix = Phi_best[best_idx]\n",
    "    best_score = best_rec_scores_all[best_idx]\n",
    "    \n",
    "    return {\n",
    "        \"best_matrix\": best_matrix,\n",
    "        \"best_score\": best_score,\n",
    "        \"best_coherence_scores\": best_coh_scores,\n",
    "        \"best_reconstruction_scores_all\": best_rec_scores_all,\n",
    "        \"best_reconstruction_scores_subset\": best_rec_scores_subset\n",
    "    }\n",
    "\n",
    "\n",
    "def find_best_reconstruction_matrices(\n",
    "    Phi_coh, validate_data, U, sparsity=0.02, num_best=50, subset_gene_indices=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Selects the best measurement matrices based on their ability to recover original gene expression patterns.\n",
    "    Now correctly extracts the third value from `compare_results()`, which is the reconstruction score.\n",
    "    \"\"\"\n",
    "    best_all = np.zeros(num_best)  \n",
    "    best_subset = np.zeros(num_best) if subset_gene_indices is not None else None\n",
    "    Phi = [None for _ in best_all]  \n",
    "\n",
    "    for i, phi in enumerate(Phi_coh):\n",
    "        y = get_observations(validate_data, phi, snr=5)\n",
    "        w = sparse_decode(y, phi.dot(U), sparsity, method='lasso')\n",
    "        x2 = U.dot(w)  \n",
    "\n",
    "        # Extract reconstruction score correctly\n",
    "        r = compare_results(validate_data, x2)\n",
    "        rec_score = r[2]  # Extract the reconstruction score\n",
    "\n",
    "        subset_score = None\n",
    "        if subset_gene_indices is not None:\n",
    "            subset_reconstruction = x2[subset_gene_indices, :]\n",
    "            subset_original = validate_data[subset_gene_indices, :]\n",
    "            subset_score = compare_results(subset_original, subset_reconstruction)[2]  # Extract score\n",
    "\n",
    "        if rec_score > best_all.min():\n",
    "            idx = np.argmin(best_all)\n",
    "            best_all[idx] = rec_score  \n",
    "            Phi[idx] = phi  \n",
    "            if subset_score is not None:\n",
    "                best_subset[idx] = subset_score  \n",
    "\n",
    "    return best_all, best_subset, Phi\n",
    "\n",
    "\n",
    "# ========== Load Gene Lists and Identify Subset Indices ==========\n",
    "genes_500 = pd.read_csv(\"./dataset/genes_500.csv\", header=None, skiprows=1)[0].tolist()\n",
    "genes_1000 = pd.read_csv(\"./dataset/genes_1000.csv\", header=None, skiprows=1)[0].tolist()\n",
    "\n",
    "# Find indices of genes_500 in genes_1000\n",
    "indices_500_in_1000 = [i for i, gene in enumerate(genes_1000) if gene in genes_500]\n",
    "\n",
    "# ========== Run Simulation for the 1000-Gene Set ==========\n",
    "simulation_results = run_simulation(\n",
    "    adata_path=\"./dataset/cerebellum/cb_mouse_1000genes.h5ad\", \n",
    "    gene_set_size=1000,\n",
    "    num_cells=10000, \n",
    "    num_measurements=50, \n",
    "    min_pools_per_gene=4, \n",
    "    max_pools_per_gene=4, \n",
    "    sparsity=0.02, \n",
    "    num_modules=50,\n",
    "    dataset_dir=\"./dataset/cerebellum\",\n",
    "    subset_gene_indices=indices_500_in_1000  # Pass the subset indices\n",
    ")\n",
    "\n",
    "# Extract reconstruction scores\n",
    "best_rec_scores_all = simulation_results[\"best_reconstruction_scores_all\"]\n",
    "best_rec_scores_subset = simulation_results[\"best_reconstruction_scores_subset\"]\n",
    "\n",
    "# Compute means\n",
    "mean_full_score = np.mean(best_rec_scores_all)\n",
    "mean_subset_score = np.mean(best_rec_scores_subset) if best_rec_scores_subset is not None else None\n",
    "\n",
    "# ========== Print Results ==========\n",
    "print(f\"Mean recovery score for all 1000 genes: {mean_full_score:.4f}\")\n",
    "if mean_subset_score is not None:\n",
    "    print(f\"Mean recovery score for the 500 genes (from 1000-gene simulation): {mean_subset_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "79603164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched genes for 1000 set: 892\n",
      "Downsampled dataset saved to ./dataset/cerebellum/cb_mouse_1000_1000genes.h5ad\n",
      "Train data shape: (50000, 892)\n",
      "Validation data shape: (25000, 892)\n",
      "Test data shape: (25000, 892)\n",
      "Datasets saved in ./dataset/cerebellum/cb_mouse_1000 as subset_data_train_data.npy, subset_data_validate_data.npy, and subset_data_test_data.npy\n",
      "Loaded dataset: subset_data\n",
      "Train data shape: (892, 50000)\n",
      "Validation data shape: (892, 25000)\n",
      "Test data shape: (892, 25000)\n",
      "n_genes = 892\n",
      "Iteration 0 of 5000\n",
      "Iteration 500 of 5000\n",
      "Iteration 1000 of 5000\n",
      "Iteration 1500 of 5000\n",
      "Iteration 2000 of 5000\n",
      "Iteration 2500 of 5000\n",
      "Iteration 3000 of 5000\n",
      "Iteration 3500 of 5000\n",
      "Iteration 4000 of 5000\n",
      "Iteration 4500 of 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projectnb/bioinfor/myousry/miniconda3/envs/comprsense/lib/python3.12/site-packages/scipy/spatial/distance.py:685: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  dist = 1.0 - uv / math.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Measurement Matrix Shape: (50, 892)\n",
      "Best Score: 0.3048025086010231\n",
      "Top 5 Coherence Scores: [0.1306926  0.12800974 0.12828587 0.12685996 0.11526894]\n",
      "Top 5 Reconstruction Scores: [0.30063431 0.30315967 0.29975268 0.29953545 0.29985891]\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "adata_path = \"dataset/cerebellum/cb_adult_mouse.h5ad\"  # Path to your AnnData file\n",
    "gene_set_size = 1000  # Choose from 500, 1000, or 5000\n",
    "num_cells = 100000  # Number of cells to include in the simulation\n",
    "num_measurements = 50  # Number of measurement pools\n",
    "min_pools_per_gene = 4  # Minimum number of pools per gene\n",
    "max_pools_per_gene = 4  # Maximum number of pools per gene\n",
    "sparsity = 0.02  # Sparsity constraint for sparse decoding\n",
    "num_modules = 50  # Number of gene modules\n",
    "dataset_dir = \"./dataset/cerebellum/cb_mouse_1000\"  # Directory where datasets are stored\n",
    "\n",
    "# Run the simulation\n",
    "results = run_simulation(\n",
    "    adata_path=adata_path,\n",
    "    gene_set_size=gene_set_size,\n",
    "    num_cells=num_cells,\n",
    "    num_measurements=num_measurements,\n",
    "    min_pools_per_gene=min_pools_per_gene,\n",
    "    max_pools_per_gene=max_pools_per_gene,\n",
    "    sparsity=sparsity,\n",
    "    num_modules=num_modules,\n",
    "    dataset_dir=dataset_dir\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(\"Best Measurement Matrix Shape:\", results[\"best_matrix\"].shape)\n",
    "print(\"Best Score:\", results[\"best_score\"])\n",
    "print(\"Top 5 Coherence Scores:\", results[\"best_coherence_scores\"][:5])\n",
    "print(\"Top 5 Reconstruction Scores:\", results[\"best_reconstruction_scores\"][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90faaca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493be56a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (comprsense)",
   "language": "python",
   "name": "comprsense"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
